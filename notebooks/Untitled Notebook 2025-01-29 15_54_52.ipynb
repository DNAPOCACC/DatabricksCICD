{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad0d564-473b-40d2-9fca-723277e608fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>current_metastore()</th></tr></thead><tbody><tr><td>azure:centralindia:0869c1b1-cade-47ff-9048-8318eeded0da</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "azure:centralindia:0869c1b1-cade-47ff-9048-8318eeded0da"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {
             "__autoGeneratedAlias": "true"
            },
            "name": "current_metastore()",
            "nullable": false,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "current_metastore()",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT CURRENT_METASTORE();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190c83c5-be31-4d80-89f5-0d2c104a08df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/databricks/connect/session.py:432: UserWarning: Ignoring the default notebook Spark session and creating a new Spark Connect session. To use the default notebook Spark session, use DatabricksSession.builder.getOrCreate() with no additional parameters.\n  warnings.warn(new_notebook_session_msg)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5257197543883703>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatabricksSession\n",
       "\u001B[0;32m----> 2\u001B[0m spark \u001B[38;5;241m=\u001B[39m DatabricksSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:435\u001B[0m, in \u001B[0;36mDatabricksSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m    433\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSDK Config profile is explicitly configured\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m Config(profile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profile)\n",
       "\u001B[0;32m--> 435\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_from_sdkconfig(\n",
       "\u001B[1;32m    436\u001B[0m         config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_user_agent(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_headers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_session_enabled)\n",
       "\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nb_session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m    439\u001B[0m     \u001B[38;5;66;03m# Running in a Databricks env - notebook/job.\u001B[39;00m\n",
       "\u001B[1;32m    440\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected running in a notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/databricks/connect/cache.py:53\u001B[0m, in \u001B[0;36mcached.<locals>._cached.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     51\u001B[0m cache_id \u001B[38;5;241m=\u001B[39m map_args_to_cache_id(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m cache \u001B[38;5;129;01mor\u001B[39;00m is_stale(cache[cache_id]):\n",
       "\u001B[0;32m---> 53\u001B[0m     cache[cache_id] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cache[cache_id]\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:502\u001B[0m, in \u001B[0;36mDatabricksSession.Builder._from_sdkconfig\u001B[0;34m(config, user_agent, headers, validate_session)\u001B[0m\n",
       "\u001B[1;32m    494\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n",
       "\u001B[1;32m    495\u001B[0m \u001B[38;5;129m@cached_session\u001B[39m(_cache_from_sdk_config\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(\u001B[38;5;28mobject\u001B[39m))\n",
       "\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_from_sdkconfig\u001B[39m(\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    500\u001B[0m     validate_session: Optional[\u001B[38;5;28mbool\u001B[39m],\n",
       "\u001B[1;32m    501\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SparkSession:\n",
       "\u001B[0;32m--> 502\u001B[0m     DatabricksSession\u001B[38;5;241m.\u001B[39mBuilder\u001B[38;5;241m.\u001B[39m_verify_sdk_version()\n",
       "\u001B[1;32m    504\u001B[0m     cluster_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    505\u001B[0m     serverless_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:581\u001B[0m, in \u001B[0;36mDatabricksSession.Builder._verify_sdk_version\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m    579\u001B[0m sdk_version \u001B[38;5;241m=\u001B[39m version(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatabricks-sdk\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Version(sdk_version) \u001B[38;5;241m<\u001B[39m Version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.29.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\u001B[0;32m--> 581\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    582\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported SDK version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msdk_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please upgrade to version 0.29.0 or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    583\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhigher. You can use `pip install --upgrade databricks-sdk`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "\u001B[0;31mException\u001B[0m: Unsupported SDK version 0.20.0. Please upgrade to version 0.29.0 or higher. You can use `pip install --upgrade databricks-sdk`"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "Exception",
        "evalue": "Unsupported SDK version 0.20.0. Please upgrade to version 0.29.0 or higher. You can use `pip install --upgrade databricks-sdk`"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>Exception</span>: Unsupported SDK version 0.20.0. Please upgrade to version 0.29.0 or higher. You can use `pip install --upgrade databricks-sdk`"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
        "File \u001B[0;32m<command-5257197543883703>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatabricks\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconnect\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatabricksSession\n\u001B[0;32m----> 2\u001B[0m spark \u001B[38;5;241m=\u001B[39m DatabricksSession\u001B[38;5;241m.\u001B[39mbuilder\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mgetOrCreate()\n",
        "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:435\u001B[0m, in \u001B[0;36mDatabricksSession.Builder.getOrCreate\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    433\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSDK Config profile is explicitly configured\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    434\u001B[0m     config \u001B[38;5;241m=\u001B[39m Config(profile\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profile)\n\u001B[0;32m--> 435\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_from_sdkconfig(\n\u001B[1;32m    436\u001B[0m         config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_user_agent(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_headers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_session_enabled)\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m nb_session \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    439\u001B[0m     \u001B[38;5;66;03m# Running in a Databricks env - notebook/job.\u001B[39;00m\n\u001B[1;32m    440\u001B[0m     logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDetected running in a notebook.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/databricks/connect/cache.py:53\u001B[0m, in \u001B[0;36mcached.<locals>._cached.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     51\u001B[0m cache_id \u001B[38;5;241m=\u001B[39m map_args_to_cache_id(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cache_id \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m cache \u001B[38;5;129;01mor\u001B[39;00m is_stale(cache[cache_id]):\n\u001B[0;32m---> 53\u001B[0m     cache[cache_id] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cache[cache_id]\n",
        "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:502\u001B[0m, in \u001B[0;36mDatabricksSession.Builder._from_sdkconfig\u001B[0;34m(config, user_agent, headers, validate_session)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    495\u001B[0m \u001B[38;5;129m@cached_session\u001B[39m(_cache_from_sdk_config\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__get__\u001B[39m(\u001B[38;5;28mobject\u001B[39m))\n\u001B[1;32m    496\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_from_sdkconfig\u001B[39m(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    500\u001B[0m     validate_session: Optional[\u001B[38;5;28mbool\u001B[39m],\n\u001B[1;32m    501\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SparkSession:\n\u001B[0;32m--> 502\u001B[0m     DatabricksSession\u001B[38;5;241m.\u001B[39mBuilder\u001B[38;5;241m.\u001B[39m_verify_sdk_version()\n\u001B[1;32m    504\u001B[0m     cluster_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    505\u001B[0m     serverless_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/spark/python/databricks/connect/session.py:581\u001B[0m, in \u001B[0;36mDatabricksSession.Builder._verify_sdk_version\u001B[0;34m()\u001B[0m\n\u001B[1;32m    579\u001B[0m sdk_version \u001B[38;5;241m=\u001B[39m version(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatabricks-sdk\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    580\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Version(sdk_version) \u001B[38;5;241m<\u001B[39m Version(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0.29.0\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 581\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    582\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnsupported SDK version \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msdk_version\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Please upgrade to version 0.29.0 or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    583\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhigher. You can use `pip install --upgrade databricks-sdk`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "\u001B[0;31mException\u001B[0m: Unsupported SDK version 0.20.0. Please upgrade to version 0.29.0 or higher. You can use `pip install --upgrade databricks-sdk`"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from databricks.connect import DatabricksSession\n",
    "spark = DatabricksSession.builder.profile(\"\").getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6722744812155829,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-01-29 15:54:52",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}